facebookå‡ºå“çš„fasttextæ¨¡å‹ã€‚å¯ä»¥ç”¨äºembeddingï¼Œä¹Ÿå¯ä»¥ç”¨äºæ–‡æœ¬åˆ†ç±»ã€‚

Fasttextå…³é”®ç‚¹ï¼š

- sub-word å’Œ n-gramï¼Œåˆ©ç”¨è¯æ ¹çº§ä¿¡æ¯å’Œè¯ç»„çº§ä¿¡æ¯ï¼Œå¹¶ä¸”å¯ä»¥testæ—¶å¤„ç†æœªå‡ºç°è¿‡çš„è¯ï¼ˆOOVï¼‰ã€‚
- hierarchical softmax å’Œ negative sampling ç”¨äºæé€Ÿã€‚



> refï¼šBag of Tricks for Efficient Text Classification https://arxiv.org/abs/1607.01759



#### fasttextçš„ ä¸»è¦åŠŸèƒ½

**Training Supervised Classifier** [supervised] Supervised Classifier Training for Text Classification. è®­ç»ƒåˆ†ç±»å™¨ï¼Œå°±æ˜¯æ–‡æœ¬åˆ†ç±»ï¼Œfasttext çš„ä¸»è¥ä¸šåŠ¡ã€‚

**Training SkipGram Model** [skipgram] Learning Word Representations/Word Vectors using skipgram technique. è®­ç»ƒskipgramçš„æ–¹å¼çš„è¯å‘é‡ã€‚

**Quantization** [quantize] Quantization is a process applied on a model so as to reduce the memory usage during prediction. é‡åŒ–å‹ç¼©ï¼Œé™ä½æ¨¡å‹ä½“ç§¯ã€‚

**Predictions** [predict] Predicting labels for a given text : Text Classification. å¯¹äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œç”¨äºé¢„æµ‹ç±»åˆ«ã€‚

**Predictions with Probabilities** [predict-prob] Predicting probabilities in addition to labels for a given text : Text Classification. å¸¦æœ‰æ¦‚ç‡çš„é¢„æµ‹ç±»åˆ«ã€‚

**Training of CBOW model** [cbow] Learning Word Representations/Word Vectors using CBOW (Continuous Bag Of Words) technique. cbowæ–¹å¼è®­ç»ƒè¯å‘é‡ã€‚

**Print Word Vectors** [print-word-vectors] Printing of Word Vectors for a trained model with each line representing a word vector. æ‰“å°ä¸€ä¸ªå•è¯çš„è¯å‘é‡ã€‚

**Print Sentence Vectors** [print-sentence-vectors] Printing of Sentence Vectors for a trained model with each line representing a vector for a paragraph. æ‰“å°æ–‡æœ¬å‘é‡ï¼Œæ¯ä¸ªæ–‡æœ¬çš„å‘é‡é•¿åº¦æ˜¯ä¸€æ ·çš„ï¼Œä»£è¡¨æ‰€æœ‰å•è¯çš„ç»¼åˆç‰¹å¾ã€‚

**Query Nearest Neighbors** [nn] æ‰¾åˆ°æŸä¸ªå•è¯çš„è¿‘é‚»ã€‚

**Query for Analogies** [analogies] æ‰¾åˆ°æŸä¸ªå•è¯çš„ç±»æ¯”è¯ï¼Œæ¯”å¦‚ A - B + Cã€‚æŸæ— - å¾·å›½ + æ³•å›½ = å·´é» è¿™ç±»çš„ä¸œè¥¿ã€‚



#### å‘½ä»¤è¡Œçš„fasttextä½¿ç”¨

1 åŸºäºè‡ªå·±çš„è¯­æ–™è®­ç»ƒword2vec

```shell
fasttext skipgram -input xxxcorpus -output xxxmodel
```

è®­ç»ƒå¾—åˆ°ä¸¤ä¸ªæ–‡ä»¶ï¼šxxxmodel.bin å’Œ xxxmodel.vecï¼Œåˆ†åˆ«æ˜¯æ¨¡å‹æ–‡ä»¶å’Œè¯å‘é‡å½¢å¼çš„æ¨¡å‹æ–‡ä»¶

å‚æ•°å¯é€‰ skipgram æˆ–è€… cbowï¼Œåˆ†åˆ«å¯¹åº”SGå’ŒCBOWæ¨¡å‹ã€‚



2 æ ¹æ®è®­ç»ƒå¥½çš„modelæŸ¥çœ‹æŸä¸ªè¯çš„neighbor

```shell
fasttext nn xxxmodel.bin
```

Query word? åè¾“å…¥å•è¯ï¼Œå³å¯è·å¾—å…¶è¿‘é‚»å•è¯ã€‚



3 å…¶å®ƒçš„ä¸€äº›å‚æ•°ï¼š

```shell
-minn å’Œ -maxn ï¼šsubwordsçš„é•¿åº¦èŒƒå›´ï¼Œdefaultæ˜¯3å’Œ6
-epoch å’Œ -lr ï¼šè½®æ•°å’Œå­¦ä¹ ç‡ï¼Œdefaultæ˜¯5å’Œ0.05
-dimï¼šè¯å‘é‡çš„ç»´åº¦ï¼Œè¶Šå¤§è¶ŠğŸ®ğŸºï¼Œä½†æ˜¯ä¼šå æ®æ›´å¤šå†…å­˜ï¼Œå¹¶é™ä½è®¡ç®—é€Ÿåº¦ã€‚
-threadï¼šè¿è¡Œçš„çº¿ç¨‹æ•°ï¼Œä¸è§£é‡Šã€‚
```



#### python æ¨¡å—çš„åº”ç”¨æ–¹å¼ï¼š

å‚æ•°å«ä¹‰ä¸åŠŸèƒ½åŸºæœ¬ç›¸åŒï¼Œç”¨æ³•å¦‚ä¸‹ã€‚

ç»™ä¸€ä¸ªæ —å­ï¼š

```python
def train_word_vector(train_fname, test_fname, epoch, lr, save_model_fname, thr):
    """
    train text classification, and save model
    """
    dim = 500               # size of word vectors [100]
    ws = 5                # size of the context window [5]
    minCount = 500          # minimal number of word occurences [1]
    minCountLabel = 1     # minimal number of label occurences [1]
    minn = 1              # min length of char ngram [0]
    maxn = 2              # max length of char ngram [0]
    neg = 5               # number of negatives sampled [5]
    wordNgrams = 2        # max length of word ngram [1]
    loss = 'softmax'              # loss function {ns, hs, softmax, ova} [softmax]
    lrUpdateRate = 100      # change the rate of updates for the learning rate [100]
    t = 0.0001                 # sampling threshold [0.0001]
    label = '__label__'             # label prefix ['__label__']

    model = fasttext.train_supervised(train_fname, lr=lr, epoch=epoch, dim=dim, ws=ws, 
                                        minCount=minCount, minCountLabel=minCountLabel,
                                        minn=minn, maxn=maxn, neg=neg, 
                                        wordNgrams=wordNgrams, loss=loss,
                                        lrUpdateRate=lrUpdateRate,
                                        t=t, label=label, verbose=True)
    model.save_model(save_model_fname)
    return model

if __name__ == "__main__":
    """ param settings """
    model = train_word_vector(train_fname, test_fname,
                              epoch, lr, save_model_fname, thr)
    model.get_nearest_neighbors(some_word)
    model.predict('sentence') # å¾—åˆ°è¾“å‡ºç±»åˆ«
    model.test(filename) # è¾“å‡ºä¸‰å…ƒç»„ï¼Œ(æ ·æœ¬æ•°é‡, acc, acc) è¿™é‡Œçš„accæ˜¯å¯¹äºŒåˆ†ç±»æ¥è¯´çš„
```





